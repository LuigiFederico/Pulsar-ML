import numpyimport scipy.optimizefrom numpy.linalg import norm import lib.model_evaluation as evdef vrow(v):    return v.reshape((1, v.size))def vcol(v):    return v.reshape((v.size, 1))################DI TEST############################################################################DI TEST############################################################################DI TEST############################################################def trainLinearSVM(DTR, LTR, K, C, DTE, p = 0):      Z = LTR * 2.0 - 1.0      X_hat = numpy.vstack([DTR, K * numpy.ones((1, DTR.shape[1]))])      G = numpy.dot(X_hat.T, X_hat)      H_hat = vcol(Z) * vrow(Z) * G      empP = (LTR == 1).sum()/len(LTR)      alphaBounds = numpy.array([(0, C)] * LTR.shape[0])            if p != 0:          alphaBounds[LTR == 1] = (0, C*p/empP)          alphaBounds[LTR == 0] = (0, C*(1-p)/(1-empP))            def computeDualLoss(alpha):             return 0.5 * numpy.dot(numpy.dot(vrow(alpha), H_hat), alpha) - alpha.sum(), numpy.dot(H_hat, alpha) - 1                def computePrimalFromDual(alpha):          w_hat = numpy.dot(alpha, (Z * X_hat).T)          w = w_hat[:-1]          b = w_hat[-1::]                          return w_hat, w, b            def computeSVMScore(w, b):          return numpy.dot(w.T, DTE) + b*K            alphaStar, x, y = scipy.optimize.fmin_l_bfgs_b(          computeDualLoss,           numpy.zeros(DTR.shape[1]),           bounds = alphaBounds,           factr=1.0,          maxfun=100000,          maxiter=100000)        w_hat, w, b = computePrimalFromDual(alphaStar)          score = computeSVMScore(w, b)      return scoredef computeDualLoss(alpha, H_hat):        return 0.5 * numpy.dot(numpy.dot(vrow(alpha), H_hat), alpha) - alpha.sum(), numpy.dot(H_hat, alpha) - 1      def computePrimalFromDual(alpha, Z, X_hat):     w_hat = numpy.dot(alpha, (Z * X_hat).T)     w = w_hat[:-1]     b = w_hat[-1::]                     return w_hat, w, b ################DI TEST############################################################################DI TEST############################################################################DI TEST############################################################ def JDual(alpha, H):    Ha = numpy.dot(H, vcol(alpha))    aHa = numpy.dot(vrow(alpha), Ha)    a1 = alpha.sum()    return -0.5* aHa.ravel() + a1, -Ha.ravel() + numpy.ones(alpha.size)   def LDual(alpha, H):    loss, grad = JDual(alpha, H)    return -loss, -graddef dualForm(DT, LT, C, K, pi_t=0):        #C is un pper bound of alpha_i.    #K. The sample x_i used into function is obtained by column vector[x_i,K],the default is 1.        DT_new = numpy.vstack([DT, K * numpy.ones((1, DT.shape[1]))])         Z = numpy.zeros(LT.shape)  # Define the Z array that will contain all the z_i values    Z[LT == 1] = 1               # Using the class label we assign 1 or -1    Z[LT == 0] = -1              # Using the class label we assign 1 or -1            H = numpy.dot(DT_new.T, DT_new)     H = vcol(Z) * vrow(Z) * H       # H_ij= z_i * z_j * x_i.T * x_j    empP = (LT == 1).sum()/(LT.shape)        x0 = numpy.zeros(LT.shape)          # See if i want the linear balanced or linear unbalanced SVM    alphaBounds = numpy.array([(0, C)] * LT.shape[0])           if pi_t != 0:         alphaBounds[LT == 1] = (0, C*pi_t/empP)         alphaBounds[LT == 0] = (0, C*(1-pi_t)/(1-empP))         # Calculate the optimization variable    best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = alphaBounds,        factr = 1.0,        maxfun=100000,        maxiter=100000)         w = numpy.dot(DT_new, vcol(best_alphas) * vcol(Z))        return wdef evaluate_linSVM(DE, wStar, K_svm):    w = (wStar.ravel())[0:-1]    b = (wStar.ravel())[-1]        S = numpy.dot(vrow(w), DE) + b*K_svm    LabelPredicted = 1*(S > 0)        return S.ravel(), LabelPredicted    def kernelPoly(DT, LT, DE, C=1, K=0, d=2, c=1):    Z = numpy.zeros(LT.shape)    Z[LT == 1] = 1    Z[LT == 0] = -1        kernel = ( numpy.dot(DT.T, DT) + c)**d    H = vcol(Z) * vrow(Z) * kernel        x0 = numpy.zeros(DT.shape[1])    b = [(0,C)] * DT.shape[1]        best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = b,        factr = 1.0,        maxfun=100000,        maxiter=100000)        # Score evaluation        s =  numpy.zeros(DE.shape[1])        for i in range(0, DE.shape[1]):        for j in range(0, DT.shape[1]):            kern = (numpy.dot(DT[:,j], DE[:, i]) + c)**d            s[i] += best_alphas[j] * Z[j] * kern         predLabels = numpy.int32(s>0)        return s, predLabels   def kernelRBF(DT, LT, DE, C=1, K=0, gamma=1):    Z = numpy.zeros(LT.shape)    Z[LT == 1] = 1    Z[LT == 0] = -1        dist = vcol((DT**2).sum(0)) + vrow((DT**2).sum(0)) - 2*numpy.dot(DT.T, DT)    kernel = numpy.exp(-gamma * dist) + K        H = vcol(Z) * vrow(Z) * kernel        x0 = numpy.zeros(DT.shape[1])    b = [(0,C)] * DT.shape[1]        best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = b,        factr = 1.0,        maxfun=100000,        maxiter=100000)        s = numpy.zeros(DE.shape[1])        for i in range(0, DE.shape[1]):        for j in range(0, DT.shape[1]):            exp = gamma * numpy.linalg.norm(DT[:,j] - DE[:,i])**2             kern = numpy.exp(-exp) + K            s[i] += best_alphas[j] * Z[j] * kern        predLabels = numpy.int32(s>0)            return s, predLabels#------------------##  SVM classifier  ##------------------#def SVM(DT, LT, DE, pi_t=0.5, prior=0.5,gamma=1.0, C=1.0, K_svm=1.0, c=0, d=2, mode="linear" ):        if (mode == "linear"):        w = dualForm(DT, LT, C, K_svm)        S, predLabels = evaluate_linSVM(DE, w, K_svm)         return S, predLabels    if (mode == "balanced-linear"):        w = dualForm(DT, LT, C, K_svm, pi_t)        S, predLabels = evaluate_linSVM(DE, w, K_svm)         return S, predLabels            if (mode == "poly"):        S, predLabels = kernelPoly(DT, LT, DE, K_svm, C, d, c)        return S, predLabels            if (mode == "RBF"):        S, predLabels = kernelRBF(DT, LT, DE, gamma, K_svm, C)        return S, predLabels    #------------------------##  Kfold implementation  ##------------------------#def kfold_SVM(k_subsets, Cs, pi_t=0.5, prior=0.5, K=5, mode='linear'):    minDCFGrapg = []    for C in Cs:      scores = []      LE = []      for i in range(K):        DT_k, LT_k = k_subsets[i][0]  # Data and Label Train        DE_k, LE_k = k_subsets[i][1]  # Data and Label Test        S, LabelPredicetd = SVM( DT_k, LT_k, DE_k, gamma=1.0, pi_t=pi_t, prior=prior, C=C, K_svm=1.0, mode=mode)        scores.append(S)         LE.append(LE_k)          LE = numpy.concatenate(LE).ravel()         scores = numpy.concatenate(scores).ravel()      minDCF = ev.computeMinDCF(LE, scores, prior, numpy.array([[0,1],[1,0]]))      minDCFGrapg.append(minDCF)      pi_tString=", pi_t = "+ str(pi_t) #Just For Balanced Case      print ("[K-Fold and", mode,"- SVM] C = ", C,             pi_tString if mode=="balanced-linear" else "",             ", prior = ", prior,             ", minDCF = ", minDCF)        return minDCFGrapg, LabelPredicetd#-------------------------------##  Single split implementation  ##-------------------------------#def single_split_SVM(split, Cs, pi_t=0.5, prior=0.5, mode='linear'):        DT, LT = split[0] # Train Data and Labels    DE, LE = split[1] # Test Data and Labels    minDCFGrapg = []        for C in Cs:        S,LabelPredicetd = SVM( DT, LT, DE, pi_t, prior, gamma=1.0, C=C, K_svm=1.0, mode=mode)        minDCF = ev.computeMinDCF(LE, S, prior, numpy.array([[0,1],[1,0]]))        minDCFGrapg.append(minDCF)        pi_tString=", pi_t = "+ str(pi_t) #Just For Balanced Case        print ("[Single_Split and", mode,"- SVM] C = ", C,               pi_tString if mode=="balanced-linear" else "",               ", prior = ", prior,               ", minDCF = ", minDCF)        return minDCFGrapg,LabelPredicetd