import numpyimport scipy.optimizefrom numpy.linalg import norm import lib.model_evaluation as evdef vrow(v):    return v.reshape((1, v.size))def vcol(v):    return v.reshape((v.size, 1))# def JDual(alpha,H):#     grad = numpy.dot(H, alpha) - numpy.ones(H.shape[1])#     return ((1/2)*numpy.dot(numpy.dot(alpha.T, H), alpha)-numpy.dot(alpha.T, numpy.ones(H.shape[1])), grad) def JDual(alpha, H):    Ha = numpy.dot(H, vcol(alpha))    aHa = numpy.dot(vrow(alpha), Ha)    a1 = alpha.sum()    return -0.5* aHa.ravel() + a1, -Ha.ravel() + numpy.ones(alpha.size)   def LDual(alpha, H):    loss, grad = JDual(alpha)    return -loss, -graddef dualForm(DT, LT, C, K):        #C is un pper bound of alpha_i.    #K. The sample x_i used into function is obtained by column vector[x_i,K],the default is 1.        # new_row = (numpy.zeros(DT.shape[1])) + K  # Define a new row of K values    # DT_new = numpy.vstack([DT, new_row])  # Define the new Dataset using the old DT and concatenating with a new row of K values        DT_new = numpy.vstack( [DT, numpy.ones((1, DT.shape[1]))] )        Z = numpy.zeros(LT.shape)  # Define the Z array that will contain all the z_i values    Z[LT == 1] = 1               # Using the class label we assign 1 or -1    Z[LT == 0] = -1              # Using the class label we assign 1 or -1            H = numpy.dot(DT_new.T, DT_new)     H = vcol(Z) * vrow(Z) * H       # H_ij= z_i * z_j * x_i.T * x_j        x0 = numpy.zeros(DT.shape[1])       b = [(0,C)] * DT.shape[1]            # Calculate the optimization variable    best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = b,        factr = 1.0)         w = numpy.dot(DT_new, vcol(best_alphas) * vcol(Z))        return wdef evaluate_linSVM(DE, wStar):    w = wStar[0,-1]    b = wStar[-1]        S = numpy.dot(vrow(w), DE) + b    LabelPredicted = 1*(S > 0)        return S.ravel(), LabelPredicted    def kernelPoly(DT, LT, DE, C=1, K=0, d=2, c=1):    Z = numpy.zeros(LT.shape)    Z[LT == 1] = 1    Z[LT == 0] = -1        kernel = ( numpy.dot(DT.T, DT) + c)**d    H = vcol(Z) * vrow(Z) * kernel        x0 = numpy.zeros(DT.shape[1])    b = [(0,C)] * DT.shape[1]        best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = b,        factr = 1.0)        # Score evaluation        s =  numpy.zeros(DE.shape[1])        for i in range(0, DE.shape[1]):        for j in range(0, DT.shape[1]):            kern = (numpy.dot(DT[:,j], DE[:, i]) + c)**d            s[i] += best_alphas[j] * Z[j] * kern         predLabels = numpy.int32(s>0)        return s, predLabels   def kernelRBF(DT, LT, DE, C=1, K=0, gamma=1):    Z = numpy.zeros(LT.shape)    Z[LT == 1] = 1    Z[LT == 0] = -1        dist = vcol((DT**2).sum(0)) + vrow((DT**2).sum(0)) - 2*numpy.dot(DT.T, DT)    kernel = numpy.exp(-gamma * dist) + K        H = vcol(Z) * vrow(Z) * kernel        x0 = numpy.zeros(DT.shape[1])    b = [(0,C)] * DT.shape[1]        best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = b,        factr = 1.0)        s = numpy.zeros(DE.shape[1])        for i in range(0, DE.shape[1]):        for j in range(0, DT.shape[1]):            exp = gamma * numpy.linalg.norm(DT[:,j] - DE[:,i])**2             kern = numpy.exp(-exp) + K            s[i] += best_alphas[j] * Z[j] * kern        predLabels = numpy.int32(s>0)            return s, predLabels                   #------------------##  SVM classifier  ##------------------#def SVM(DT, LT, DE, gamma=1.0, C=1.0, K=1.0, c=0, d=2, mode="linear" ):        if (mode == "linear"):        w = dualForm(DT, LT, C, K)        # DE = numpy.vstack([DE, numpy.zeros(DE.shape[1]) + K])        # S = numpy.dot(w.T, DE) # +b ?        # LabelPredicted = 1*(S > 0)        S, predLabels = evaluate_linSVM(DE, w)         return S, predLabels            if (mode == "poly"):        S, predLabels = kernelPoly(DT, LT, DE, K, C, d, c)        return S, predLabels            if (mode == "RBF"):        S, predLabels = kernelRBF(DT, LT, DE, gamma, K, C)        return S, predLabels    #------------------------##  Kfold implementation  ##------------------------#def kfold_SVM(k_subsets, prior=0.5, K=5, mode='linear'):    Cs = numpy.logspace(-1, -1, num=1)  #For Normal Use    #Cs = numpy.logspace(-3, 1, num=30) #For Graphichs    minDCFGrapg = []    for C in Cs:      scores = []      LE = []      for i in range(K):        DT_k, LT_k = k_subsets[i][0]  # Data and Label Train        DE_k, LE_k = k_subsets[i][1]  # Data and Label Test        S,_ = SVM( DT_k, LT_k, DE_k, gamma=1.0, C=C, K=1.0, mode=mode)        scores.append(S)         LE.append(LE_k)          LE = numpy.concatenate(LE).ravel()         scores = numpy.concatenate(scores).ravel()      minDCF = ev.computeMinDCF(LE, scores, prior, numpy.array([[0,1],[1,0]]))      minDCFGrapg.append(minDCF)      print ("[Single_Split and Linear SVG] C = ", C,             ", prior = ", prior,             ", minDCF = ", minDCF)        return minDCFGrapg, Cs#-------------------------------##  Single split implementation  ##-------------------------------#def single_split_SVM(split, prior=0.5, mode='linear'):        DT, LT = split[0] # Train Data and Labels    DE, LE = split[1] # Test Data and Labels    Cs = numpy.logspace(-1, -1, num=1)  #For Normal Use    #Cs = numpy.logspace(-3, 1, num=30) #For Graphichs    minDCFGrapg = []        for C in Cs:        S,_ = SVM( DT, LT, DE, gamma=1.0, C=C, K=1.0, mode=mode)        minDCF = ev.computeMinDCF(LE, S, prior, numpy.array([[0,1],[1,0]]))        minDCFGrapg.append(minDCF)        print ("[Single_Split and Linear SVG] C = ", C,               ", prior = ", prior,               ", minDCF = ", minDCF)        return minDCFGrapg, Cs