import numpyimport scipy.optimizefrom numpy.linalg import norm import lib.model_evaluation as evdef vrow(v):    return v.reshape((1, v.size))def vcol(v):    return v.reshape((v.size, 1))     def JDual(alpha, H):    Ha = numpy.dot(H, vcol(alpha))    aHa = numpy.dot(vrow(alpha), Ha)    a1 = alpha.sum()    return -0.5* aHa.ravel() + a1, -Ha.ravel() + numpy.ones(alpha.size)   def LDual(alpha, H):    loss, grad = JDual(alpha, H)    return -loss, -graddef dualForm(DT, LT, C, K, pi_t=0):        #C is un pper bound of alpha_i.    #K. The sample x_i used into function is obtained by column vector[x_i,K],the default is 1.        DT_new = numpy.vstack([DT, K * numpy.ones((1, DT.shape[1]))])         Z = numpy.zeros(LT.shape)  # Define the Z array that will contain all the z_i values    Z[LT == 1] = 1               # Using the class label we assign 1 or -1    Z[LT == 0] = -1              # Using the class label we assign 1 or -1            H = numpy.dot(DT_new.T, DT_new)     H = vcol(Z) * vrow(Z) * H       # H_ij= z_i * z_j * x_i.T * x_j    empP = (LT == 1).sum()/(LT.shape)        x0 = numpy.zeros(LT.shape)          # See if i want the linear balanced or linear unbalanced SVM    alphaBounds = numpy.array([(0, C)] * LT.shape[0])           if pi_t != 0:         alphaBounds[LT == 1] = (0, C*pi_t/empP)         alphaBounds[LT == 0] = (0, C*(1-pi_t)/(1-empP))         # Calculate the optimization variable    best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = alphaBounds,        factr = 1.0,        maxfun=100000,        maxiter=100000)         w = numpy.dot(DT_new, vcol(best_alphas) * vcol(Z))        return wdef evaluate_linSVM(DE, wStar, K_svm):    w = (wStar.ravel())[0:-1]    b = (wStar.ravel())[-1]        S = numpy.dot(vrow(w), DE) + b*K_svm    LabelPredicted = 1*(S > 0)        return S.ravel(), LabelPredicted    def kernelPoly(DT, LT, DE, C=0.1, K=0, d=2, c=1):    Z = numpy.zeros(LT.shape)    Z[LT == 1] = 1    Z[LT == 0] = -1        kernel = ( numpy.dot(DT.T, DT) + c)**d    H = vcol(Z) * vrow(Z) * kernel        x0 = numpy.zeros(DT.shape[1])    b = [(0,C)] * DT.shape[1]        best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = b,        factr = 1.0,        maxfun=100000,        maxiter=100000)            s = numpy.sum(numpy.dot((best_alphas*LT).reshape(1, DT.shape[1]), (numpy.dot(DT.T, DE)+c)**d+ K), axis=0)         predLabels = numpy.int32(s>0)        return s, predLabels   def kernelRBF(DT, LT, DE, C=1, K=0, gamma=0.01):    Z = numpy.zeros(LT.shape)    Z[LT == 1] = 1    Z[LT == 0] = -1        dist = vcol((DT**2).sum(0)) + vrow((DT**2).sum(0)) - 2*numpy.dot(DT.T, DT)    kernel = numpy.exp(-gamma * dist) + K**2        H = vcol(Z) * vrow(Z) * kernel        x0 = numpy.zeros(DT.shape[1])    b = [(0,C)] * DT.shape[1]        best_alphas, _x, _y = scipy.optimize.fmin_l_bfgs_b(        LDual,        x0,        args=(H,),        bounds = b,        factr = 1.0,        maxfun=100000,        maxiter=100000)          s = numpy.zeros(DE.shape[1])          Dist = vcol((DT**2).sum(0)) + vrow((DE**2).sum(0)) - 2 * numpy.dot(DT.T, DE)    k = numpy.exp(-gamma*Dist)  + K**2     s = numpy.dot((vcol(best_alphas) * vcol(Z)).T,k)    s.sum(0)    s=s.ravel()        predLabels = numpy.int32(s>0)            return s, predLabels#------------------##  SVM classifier  ##------------------#def SVM(DT, LT, DE, pi_t=0.5, prior=0.5, gamma=0.01, C=1.0, K_svm=1.0, c=0, d=2, mode="linear" ):    if (mode == "linear"):        w = dualForm(DT, LT, C, K_svm)        S, predLabels = evaluate_linSVM(DE, w, K_svm)         return S, predLabels    if (mode == "balanced-linear"):        w = dualForm(DT, LT, C, K_svm, pi_t)        S, predLabels = evaluate_linSVM(DE, w, K_svm)         return S, predLabels            if (mode == "poly"):        S, predLabels = kernelPoly(DT, LT, DE, C=C, K=K_svm, d=d, c=c)        return S, predLabels            if (mode == "RBF"):        S, predLabels = kernelRBF(DT, LT, DE, C=C, K=K_svm, gamma=gamma)        return S, predLabels    #------------------------##  Kfold implementation  ##------------------------#def kfold_SVM(k_subsets, Cs, pi_t=0.5, prior=[0.5], K=5, mode='linear', gamma=0.01, c=0, getScores=False):    minDCFGrapg = []    scores_final = []    for p in prior:        for C in Cs:          scores = []          LE = []          for i in range(K):            DT_k, LT_k = k_subsets[i][0]  # Data and Label Train            DE_k, LE_k = k_subsets[i][1]  # Data and Label Test            S, LabelPredicetd = SVM( DT_k, LT_k, DE_k, pi_t=pi_t, prior=p,gamma=gamma, C=C, K_svm=1.0, c=c, mode=mode)            scores.append(S)             LE.append(LE_k)                  LE = numpy.concatenate(LE).ravel()             scores = numpy.concatenate(scores).ravel()          if getScores: scores_final.append(vcol(scores))          else:              minDCF = ev.computeMinDCF(LE, scores, p, numpy.array([[0,1],[1,0]]))              minDCFGrapg.append(minDCF)                            pi_tString=", pi_t = "+ str(pi_t) #Just For Balanced Case              c_string=", c ="+str(c) # poly              gamma_String=", gamma = "+ str(gamma)#Just For RBF Case              print ("[K-Fold and", mode,"- SVM] C = ", C,                pi_tString if mode=="balanced-linear" else "",                c_string if mode=="poly" else "",                gamma_String if mode=="RBF" else "",                ", prior = ", p,                ", minDCF = ", minDCF)            if getScores:        return scores_final, LE    return minDCFGrapgdef kfold_SVM_actDCF(k_subsets, C, pi_t=0.5, prior=[0.5], K=5, mode='linear', gamma=0.01, c=0):    actDCF_final = []    minDCF_final = []    for p in prior:        scores = []        LE = []        for i in range(K):            DT_k, LT_k = k_subsets[i][0]  # Data and Label Train            DE_k, LE_k = k_subsets[i][1]  # Data and Label Test            S, LabelPredicetd = SVM( DT_k, LT_k, DE_k, pi_t, p,gamma, C, K_svm=1.0, c=c, mode=mode)            scores.append(S)             LE.append(LE_k)            LE = numpy.concatenate(LE).ravel()           scores = numpy.concatenate(scores).ravel()        actDCF = ev.computeActualDCF(LE, scores, p, 1, 1)        minDCF = ev.computeMinDCF(LE, scores, p, numpy.array([[0,1],[1,0]]))        minDCF_final.append(minDCF)        actDCF_final.append(actDCF)        print(' SVM (C=0.1, pi_T=0.5, prior=%.1f): actDCF=%.3f  &  minDCF=%3f'          % (p, actDCF, minDCF))        return actDCF_final, minDCF_finaldef kfold_SVM_actDCF_Calibrated(k_subsets, C, pi_t=0.5, prior=[0.5], K=5, mode='linear', lambd_calib=1e-4, getScores=False):    actDCF_final = []    scores_final = []    for p in prior:        scores = []        LE = []        for i in range(K):            DT_k, LT_k = k_subsets[i][0]  # Data and Label Train            DE_k, LE_k = k_subsets[i][1]  # Data and Label Test            S, LabelPredicetd = SVM( DT_k, LT_k, DE_k, pi_t, p, C=C, K_svm=1.0, mode=mode)            scores.append(S)             LE.append(LE_k)            LE = numpy.concatenate(LE).ravel()           scores = numpy.concatenate(scores).ravel()        scores = ev.calibrateScores(scores,LE,lambd_calib,p)        if getScores: scores_final.append(vcol(scores))        actDCF = ev.computeActualDCF(LE, scores, p, 1, 1)        actDCF_final.append(actDCF)        if getScores:        return scores_final, LE        return actDCF_final#-------------------------------##  Single split implementation  ##-------------------------------#def single_split_SVM(split, Cs, pi_t=0.5, prior=[0.5], mode='linear'):        DT, LT = split[0] # Train Data and Labels    DE, LE = split[1] # Test Data and Labels    minDCFGrapg = []        for p in prior:        for C in Cs:            S,_ = SVM( DT, LT, DE, pi_t, prior, gamma=1.0, C=C, K_svm=1.0, mode=mode)            minDCF = ev.computeMinDCF(LE, S, prior, numpy.array([[0,1],[1,0]]))            minDCFGrapg.append(minDCF)                        pi_tString=", pi_t = "+ str(pi_t) #Just For Balanced Case            print ("[Single_Split and", mode,"- SVM] C = ", C,                   pi_tString if mode=="balanced-linear" else "",                   ", prior = ", prior,                   ", minDCF = ", minDCF)        return minDCFGrapg, _    def SVM_EVALUATION(split, C, pi_t, prior, lambd_calib=1e-4, mode='linear'):        DT, LT = split[0] # Train Data and Labels    DE, LE = split[1] # Test Data and Labels    minDCF_final = []    actDCF_final = []    actDCFCalibrated_final = []        for p in prior:        S,_ = SVM( DT, LT, DE, pi_t, prior, gamma=1.0, C=C, K_svm=1.0, mode=mode)        actDCF = ev.computeActualDCF(LE, S, p, 1, 1) # Compute the actDCF        minDCF = ev.computeMinDCF(LE, S, p, numpy.array([[0,1],[1,0]]))                        S_train,_=SVM( DT, LT, DT, pi_t, prior, gamma=1.0, C=C, K_svm=1.0, mode=mode)        S_Calib = ev.calibrateScoresForEvaluation(S_train,LT,S,lambd_calib,p)        actDCFCalibrated = ev.computeActualDCF(LE, S_Calib, p, 1, 1) # Compute the actDCF            minDCF_final.append(minDCF)        actDCF_final.append(actDCF)        actDCFCalibrated_final.append(actDCFCalibrated)        if mode == "linear":            print ("SVM - Linear with C = %.2f , prior = %.1f , minDCF = %.3f , actDCF = %.3f, actDCF (Calibrated) = %.3f" % (C,p,minDCF,actDCF,actDCFCalibrated))        else:            print ("SVM - Balanced Linear with C = %.2f , pi_T = %.1f , prior = %.1f , minDCF = %.3f , actDCF = %.3f, actDCF (Calibrated) = %.3f" % (C,pi_t,p,minDCF,actDCF,actDCFCalibrated))        return minDCF_final, actDCF_final, actDCFCalibrated_final