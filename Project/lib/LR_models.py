import numpyimport scipy.optimizefrom numpy.linalg import norm import lib.model_evaluation as evdef vrow(v):    return v.reshape((1, v.size))def vcol(v):    return v.reshape((v.size, 1))def Jgradrebalanced(w, b, DT, LT, lambd, prior):        Z=vcol((LT*(2.0)) - 1.0)        term_a=(lambd/2)*(pow(norm(w,2),2))  # Normalization term equal to (lambda/2)*(||w||^2)        sum_T = 0                            # Sum of terms of True Class    sum_F = 0                            # Sum of terms of False Class    for i in range(DT.shape[1]):                z_i=Z[i,0]        arg_exp = -z_i*(numpy.dot(w.T, DT[:, i]) + b)        if LT[i]==1:             sum_T += numpy.log1p(numpy.exp(arg_exp))        else:            sum_F += numpy.log1p(numpy.exp(arg_exp))        J = term_a + (prior/DT[:, LT==1].shape[1])*sum_T + ((1-prior)/DT[:, LT==0].shape[1])*sum_F    return Jdef logreg_obj(v, DT, LT, lambd, prior=0.5):    w, b = v[0:-1], v[-1]        j = Jgradrebalanced(w, b, DT, LT, lambd, prior)        return jdef expand_feature_space(D):    def vec_xxT(x):        x = x[:, None]        x_xT = x.dot(x.T).reshape(x.size**2, order='F')           return x_xT        # Apply a function to every column    expanded = numpy.apply_along_axis(vec_xxT, 0, D)        return numpy.vstack([expanded, D])#----------------------##  Log Reg classifier  ##----------------------#def LogReg( DT, LT, DE, TE, lambd, prior=0.5):    x0=numpy.zeros(DT.shape[0] + 1)        _x, _f, _d = scipy.optimize.fmin_l_bfgs_b(logreg_obj, x0, args=(DT, LT, lambd, prior), approx_grad=True)    optim_w=_x[0:-1]    optim_b=_x[-1]        S=(numpy.dot(optim_w.T,DE)+optim_b)    LabelPredicted=(S>0).astype(int)        return S,LabelPredicted#------------------------##  Kfold implementation  ##------------------------#def kfold_LogReg(k_subsets, K, prior=0.5):        #lambdas=numpy.logspace(-5, 2, num=30)  #For Graphichs    lambdas=numpy.logspace(-5,-5,1)         #For Normal Use    minDCFGrapg= []    for lambd in lambdas:      scores = []      LE = []      for i in range(K):        DT_k, LT_k = k_subsets[i][0]  # Data and Label Train        DE_k, LE_k = k_subsets[i][1]  # Data and Label Test        S, LabelPredicted = LogReg( DT_k, LT_k, DE_k, LE_k, lambd, prior=0.5) # Classify the DE_k data        scores.append(S)         LE.append(LE_k)          LE = numpy.concatenate(LE).ravel()         scores = numpy.concatenate(scores).ravel()      minDCF = ev.computeMinDCF(LE, scores, prior, numpy.array([[0,1],[1,0]])) # Compute the minDCF      minDCFGrapg.append(minDCF)      print ("[K_Fold] For Lambda value: ", lambd,             " and Prior value equal to: ", prior,             " the minDCF value is: ", minDCF)        return minDCFGrapg, lambdas def kfold_QuadLogReg(k_subsets, K, prior=0.5):            #lambdas=numpy.logspace(-5, 2, num=30)  #For Graphichs    lambdas=numpy.logspace(-5,-5,1)         #For Normal Use    minDCFGrapg= []    for lambd in lambdas:      scores = []      LE = []      for i in range(K):        DT_k, LT_k = k_subsets[i][0]  # Data and Label Train        DE_k, LE_k = k_subsets[i][1]  # Data and Label Test        DT_k = expand_feature_space(DT_k)        DE_k = expand_feature_space(DE_k)                S, LabelPredicted = LogReg( DT_k, LT_k, DE_k, LE_k, lambd, prior=0.5) # Classify the DE_k data        scores.append(S)         LE.append(LE_k)          LE = numpy.concatenate(LE).ravel()         scores = numpy.concatenate(scores).ravel()      minDCF = ev.computeMinDCF(LE, scores, prior, numpy.array([[0,1],[1,0]])) # Compute the minDCF      minDCFGrapg.append(minDCF)      print ("[K_Fold] For Lambda value: ", lambd,             " and Prior value equal to: ", prior,             " the minDCF value is: ", minDCF)        return minDCFGrapg, lambdas #-------------------------------##  Single split implementation  ##-------------------------------#def single_split_LogReg(split, prior=0.5):        DT, LT = split[0] # Train Data and Labels    DE, LE = split[1] # Test Data and Labels    #lambdas=numpy.logspace(-5, 2, num=30)  #For Graphichs    lambdas=numpy.logspace(-5,-5,1)         #For Normal Use    minDCFGrapg=[]        for lambd in lambdas:        S,LabelPredicted = LogReg( DT, LT, DE, LE, lambd, prior=0.5) # Classify the DE_k data        minDCF = ev.computeMinDCF(LE, S, prior, numpy.array([[0,1],[1,0]]))        minDCFGrapg.append(minDCF)        print ("[Single_Split] For Lambda value: ", lambd,               " and Prior value equal to: ", prior,               " the minDCF value is: ", minDCF)            return minDCFGrapg, lambdasdef single_split_QuadLogReg(split, prior=0.5):        DT, LT = split[0] # Train Data and Labels    DE, LE = split[1] # Test Data and Labels    DT = expand_feature_space(DT)    DE = expand_feature_space(DE)    #lambdas=numpy.logspace(-5, 2, num=30)  #For Graphichs    lambdas=numpy.logspace(-5,-5,1)         #For Normal Use    minDCFGrapg=[]        for lambd in lambdas:        S,LabelPredicted = LogReg( DT, LT, DE, LE, lambd, prior=0.5) # Classify the DE_k data        minDCF = ev.computeMinDCF(LE, S, prior, numpy.array([[0,1],[1,0]]))        minDCFGrapg.append(minDCF)        print ("[Single_Split] For Lambda value: ", lambd,               " and Prior value equal to: ", prior,               " the minDCF value is: ", minDCF)            return minDCFGrapg, lambdas